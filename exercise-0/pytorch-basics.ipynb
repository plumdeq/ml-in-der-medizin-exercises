{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0: Basic pytorch\n",
    "\n",
    "*authors: Asan Agibetov, Georg Dorffner*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the exercises create a cell below the definition of the exercise and insert your \"code\" there. Make sure that it runs and prints out the expected result. \n",
    "\n",
    "*Use `Insert -> Insert Cell Below` Jupyter Notebook menu item to insert cells below the currently selected one.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "1. Tensor manipulation with numpy and pytorch\n",
    "2. Computational graphs and gradient computation\n",
    "3. Training a simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tensor manipulation with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines a simple vector $\\vec{x_1} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "x_1 = np.array([1, 2 ,3])\n",
    "print(x_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elementwise operations on vectors $\\vec{x_2} = 3 \\vec{x_1} = 3 \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 6 9]\n"
     ]
    }
   ],
   "source": [
    "x_2 = x_1 * 3\n",
    "print(x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define matrices with dimensions $m \\times n$ or even tensors with arbitrary dimensions $m_1 \\times \\cdots \\times m_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]\n",
      " [4 5 6]] (3, 3)\n"
     ]
    }
   ],
   "source": [
    "A_1 = np.array([[1, 2, 3], [2, 3, 4], [4, 5, 6]])\n",
    "print(A_1, A_1.shape) # a 3 x 3 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 2 3]\n",
      "  [2 3 4]\n",
      "  [4 5 6]]\n",
      "\n",
      " [[1 2 3]\n",
      "  [2 3 4]\n",
      "  [4 5 6]]\n",
      "\n",
      " [[1 2 3]\n",
      "  [2 3 4]\n",
      "  [4 5 6]]] (3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "A_2 = np.array([A_1, A_1, A_1])\n",
    "print(A_2, A_2.shape) # a 3 x 3 x 3 tensor, i.e., three A_1 matrices stacked together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "\n",
    "Create a cell below and define a tensor with dimensions $3 \\times 3 \\times 3$ consisting of transformed matrices $\\begin{pmatrix} 3A_1 \\\\ 2A_1 + A_1 \\\\ 10A_1 - 2 \\end{pmatrix}$. All operations are elementwise, scalars are \"broadcast\" to all elements in the tensor\n",
    "\n",
    "```python\n",
    "np.array([1, 2]) - 1\n",
    ">>> [0, 1] # i.e., will subtract 1 from all the elements in the Numpy array.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tensor manipulation with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch tensors can be either created manually or from numpy tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  2  3\n",
      " 2  3  4\n",
      " 3  4  5\n",
      "[torch.LongTensor of size 3x3]\n",
      " torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "X_1 = torch.Tensor([[1, 2, 3], [2, 3, 4], [3, 4, 5]]).long()\n",
    "print(X_1, X_1.size()) # 3 x 3 Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  0  0\n",
      " 0  0  0\n",
      "-1 -1 -1\n",
      "[torch.LongTensor of size 3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_2 = torch.from_numpy(A_1)\n",
    "print(X_1 - X_2) # subtraction of matrices, notice how we create pytorch tensors from numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "\n",
    "Create a pytorch tensor from the numpy tensor that you have created in `Exercise 1.1` and multiply all its elements by 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computational graphs and gradient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of using pytorch tensors over numpy tensors, is that we can benefit from the `auograd` pytorch package, which computes gradients automatically for us. To demonstrate it, let us first define a multi-variable function\n",
    "\n",
    "$$\n",
    "y = f(w, x, b) = wx + b,\n",
    "$$\n",
    "\n",
    "where $w, x, b$ are all variables (i.e., $w, b$ are not constants!). We transform the definition of $y$ into its *computational graph* representation with `pytorch` as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 5\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "b = Variable(torch.Tensor([3]), requires_grad=True)\n",
    "\n",
    "y = w * x + b\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use `torch.autograd.Variable` to indicate that $w, x, b$ will vary over different values, and we indicate that we want to compute gradients with respect to these variables (`requires_grad=True`). Above we evaluate y at the point $(1, 2, 3)$ in the $wxb$ plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "\n",
    "Evaluate function $y$ at the point $(2, 3, 5)$. Re-use the defined `pytorch` variables $w, x, b$ by passing them tensors that correspond to the point $(2, 3, 5)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the *computational graph* `y` and the variables `w, x, b`, we can now compute the gradient of `f(w, x, b)`\n",
    "\n",
    "$$\n",
    "\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial w} \\\\ \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial b} \\end{bmatrix} = \\begin{bmatrix} x \\\\ w \\\\ 1 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Evaluated at the point $(1, 2, 3)$ the gradient $\\nabla f(1, 2, 3)$ is the vector $\\begin{bmatrix} 2 \\\\ 1 \\\\ 1 \\end{bmatrix}$. We use `backward` method to automatically compute the gradients in `pytorch`. \n",
    "\n",
    "**You might need to rerun the cell with the definition of the computational graph `y`, if you call `backward` more than once.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "\n",
    "print(w.grad) # partial f / partial w\n",
    "print(x.grad) # partial f / partial x\n",
    "print(b.grad) # partial f / partial b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "\n",
    "Define a computational graph for the function $g(w, x, b) = wx^2 + b$. Compute its gradients at the point $(2, 3, 5)$.\n",
    "\n",
    "Use `x*x` for $x^2$ in pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main ingredient of Neural Networks are hidden layers, which implement simple non-linear functions that transform input into the output. `pytorch` uses a notion of *modules* - that can be found in `torch.nn` package - to compositionally build hidden layers. For instance, below we define a linear layer $wx + b$, where $w$ are weight and $b$ are bias terms, which transforms points in 3 dimensions into the points in 2 dimensions $\\mathbb{R}^3 \\mapsto \\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "-0.2772 -0.0442 -0.5263\n",
      "-0.0435 -0.0808  0.1451\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "Parameter containing:\n",
      "-0.2353\n",
      "-0.0697\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(3, 2)\n",
    "print(linear.weight)\n",
    "print(linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the weight matrix has dimensions $W: 2 \\times 3$, such that whenever applied to a vector $v \\in \\mathbb{R}^3$ it will transform it into a vector in $Wv \\in \\mathbb{R}^2$. Analogously, $b \\in \\mathbb{R}^2$, so that $Wx + b \\in \\mathbb{R}^2$ is possible. To add *non-linearity* we will use a *sigmoid* function\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "in composition with our linear *module* - $\\sigma(wx + b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid() \n",
    "# note that the dimensions will be determined automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a random tensor $X_3$ of dimension $5 \\times 3$, containing 5 3-dimensional points $x_i$. And apply our simple non-linear module to transform it into a tensor of dimensions $5 \\times 2$, i.e., each 3-dimensional point $x_i$ is going to be transformed into a 2-dimensional point.\n",
    "\n",
    "*To apply `pytorch` module to tensors, you need to wrape them in `Variable`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turning\n",
      "Variable containing:\n",
      "-0.6649  1.0590  0.0124\n",
      "-0.1849  0.8260 -0.6836\n",
      " 1.0338 -0.7731  1.9002\n",
      " 0.5367  2.8721 -0.5889\n",
      "-1.4346  1.0805  1.3276\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "into\n",
      "Variable containing:\n",
      " 0.4740  0.4689\n",
      " 0.5348  0.4433\n",
      " 0.1843  0.5557\n",
      " 0.4499  0.3988\n",
      " 0.3580  0.5245\n",
      "[torch.FloatTensor of size 5x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_3 = Variable(torch.randn(5, 3)) # torch.randn creates a 5 x 3 tensor with random values in it\n",
    "print(\"turning\")\n",
    "print(X_3)\n",
    "# notice how the mathematical composition defined above is translated into the composition of pytorch modules\n",
    "print(\"into\")\n",
    "print(sigmoid(linear(X_3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you will have to write compositions of modules likes this very often; `pytorch` provides a convenience function `torch.nn.Sequence` which composes a list of modules into one module, i.e., it performs function composition $f = x \\circ y \\circ z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.4740  0.4689\n",
       " 0.5348  0.4433\n",
       " 0.1843  0.5557\n",
       " 0.4499  0.3988\n",
       " 0.3580  0.5245\n",
       "[torch.FloatTensor of size 5x2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(linear, sigmoid)\n",
    "net(X_3) # equivalent to running sigmoid(linear(X_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1 \n",
    "\n",
    "Use `torch.nn.ReLU` module in combination of our previosly defined linear module and apply their composition to the tensor $X_3$.\n",
    "\n",
    "*Note that you first need to create an instance of the module, and then apply that instance to Variables**\n",
    "\n",
    "```python\n",
    "torch.nn.Sigmoid(Variable(torch.randn(5, 3))) # won't work\n",
    "\n",
    "# instead use instances\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "sigmoid(Variable(torch.randn(5, 3)))\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will use our hidden layer with the sigmoid activation function to teach a Neural Network to map vectors in $\\mathbb{R}^3$ into vectors in $\\mathbb{R}^2$. We will simulate a function $h: \\mathbb{R}^3 \\mapsto \\mathbb{R}^2$ with 1000 random samples for input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = Variable(torch.randn(1000, 3))\n",
    "OUTPUT = Variable(torch.randn(1000, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning can be decomposed into four main steps:\n",
    "\n",
    "1. forward propagation\n",
    "    * getting the currently learnt output of the network - prediction\n",
    "2. loss error computation\n",
    "    * measuring how far off is the prediction from the true output\n",
    "3. gradient computation via backpropagation\n",
    "    * computing gradients with respect to all the inputs in the network\n",
    "4. optimization and parameter update\n",
    "    * updating parameters of the network in the opposite direction of their gradients, so that the loss error is minimized in the next iteration\n",
    "    \n",
    "For our example, we will use *mean squared error* as our loss error and *stochastic gradient descent* as our optimization algorithm. We will set the learning rate - how fast we are moving in the opposite direction of the gradient - to 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propagation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply apply our network to all the inputs $\\sigma(wx_i + b), x_i \\in input$. Notice that the `pytorch` modules allow us to perform *vectorized* operations, i.e., perform operations for all inputs and outputs in one run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.4150  0.5330\n",
      " 0.4788  0.4425\n",
      " 0.2449  0.5359\n",
      "       â‹®        \n",
      " 0.4414  0.4769\n",
      " 0.4471  0.5114\n",
      " 0.4762  0.4766\n",
      "[torch.FloatTensor of size 1000x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = net(INPUT) # transform all 1000 R3 vectors into 1000 R2 vectors\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss error computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the mean squared error between the predictions for all the inputs $x_i \\in input$ and the true outputs $y_i \\in output$\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{n} \\sum_i \\lVert y_i - f(x_i) \\rVert_2 ^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.2629362344741821\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(prediction, OUTPUT)\n",
    "print(\"loss: \", loss.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient computation via backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the gradients $\\frac{\\partial L}{\\partial w}, \\frac{\\partial L}{\\partial b}$ are \"collected\" in the parameters - weights and biases - of the modules of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "1.00000e-02 *\n",
      " -0.5875 -1.5929 -2.8871\n",
      " -0.9636 -1.3763  0.3484\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "Variable containing:\n",
      " 0.0989\n",
      " 0.1254\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linear_module, sigmoid_module = net.children()\n",
    "print(linear_module.weight.grad)\n",
    "print(linear_module.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`optimizer.step()` is performing automatic update of all the parameters with the gradients (scaled by the learning rate $\\nu$). \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_i' &:= w_i - \\nu \\frac{\\partial L}{\\partial w_i} \\\\\n",
    "b_i' &:= b_i - \\nu \\frac{\\partial L}{\\partial b_i}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Behind the scenes it is performing updates to the weights and biases of each `pytorch` module. For instance, you could have updated the weights and the biases for the linear module manually as follows:\n",
    "\n",
    "```python\n",
    "linear_module.weight.data.sub_(lr * linear_module.weight.grad.data)\n",
    "linear_module.bias.data.sub_(lr * linear_module.bias.grad.data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 epoch of optimization: 1.2626652717590332\n"
     ]
    }
   ],
   "source": [
    "prediction_after_one_epoch = net(INPUT)\n",
    "loss_after_one_epoch = criterion(prediction_after_one_epoch, OUTPUT)\n",
    "print(\"loss after 1 epoch of optimization: {}\".format(\n",
    "    loss_after_one_epoch.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "\n",
    "Below we showed how the Neural Network can be trained to learn how to map vectors from 3- to 2- dimensional spaces, by performing forward propagation, loss computation and backpropagation for the parameter update. We did all that only once. In practice, Neural Networks are trained for many epochs (thousands), i.e., the above steps are repeated until the `loss` converges (if possible). In this exercise we ask you to train the Neural Network `net` for bigger numbers of epochs, for instance for 100 epochs. Try training for more epochs until the loss converges to values close to zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
