{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical exercise\n",
    "\n",
    "## Machine Learning in der Medizin\n",
    "\n",
    "*authors: Asan Agibetov, Geord Dorffner*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard-library imports\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Normal\n",
    "import click\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb2b99e1510>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Approxnet(nn.Module):\n",
    "    def __init__(self, input_dim, hids_dims, out_dim, activation=\"relu\"):\n",
    "        super(Approxnet, self).__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.activation_fn = F.relu if activation == \"relu\" else F.sigmoid\n",
    "\n",
    "        self.input = nn.Linear(input_dim, hids_dims[0])\n",
    "        \n",
    "        self.hids = nn.ModuleList(\n",
    "                [nn.Linear(h1, h2) for h1, h2 in zip(hids_dims[:-1], hids_dims[1:])])\n",
    "        self.output = nn.Linear(hids_dims[-1], out_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.input.weight.data.normal_(0, 1./math.sqrt(self.input.weight.size(1)))\n",
    "        self.input.bias.data.zero_()\n",
    "        \n",
    "        for hid in self.hids:\n",
    "            hid.weight.data.normal_(0, 1./math.sqrt(hid.weight.size(1)))\n",
    "            hid.bias.data.zero_()\n",
    "        \n",
    "        self.output.weight.data.normal_(0, 1./math.sqrt(self.output.weight.size(1)))\n",
    "        self.output.bias.data.zero_()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden=False):\n",
    "        \"\"\"Output activations of the last hidden neuron layer\"\"\"\n",
    "        output = self.activation_fn(self.input(x))\n",
    "\n",
    "        for hid in self.hids:\n",
    "            output = self.activation_fn(hid(output))\n",
    "\n",
    "        if hidden:\n",
    "            return output\n",
    "\n",
    "        # if self.activation == \"sigmoid\":\n",
    "        #    return self.activation_fn(self.output(output))\n",
    "\n",
    "        return self.output(output)\n",
    "\n",
    "\n",
    "    def max_weight_neurons(self):\n",
    "        \"\"\"Sort neuron activity by the weights\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def min_weight_neurons(self):\n",
    "        \"\"\"Sort neuron activity by the weights\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples):\n",
    "    # Generate 100 random points, the values we want to approximate\n",
    "    EPS = 1e-5\n",
    "    x = torch.linspace(-2*math.pi, 2*math.pi, n_samples).view(-1, 1)\n",
    "    x_generate = torch.from_numpy(np.random.uniform(-2*math.pi, 2*math.pi, (n_samples,))).view(-1, 1).float()\n",
    "    \n",
    "    true_function = lambda x: torch.cos(x)**2 + torch.cos(x)**3 + torch.sin(x)**5\n",
    "    \n",
    "    y = true_function(x_generate)\n",
    "    # y = torch.cos(x) + 1.2\n",
    "    \n",
    "    # y /= y.max()\n",
    "    # y += y.min() + EPS\n",
    "    \n",
    "    mean_y, std_y = y.mean(), y.std()\n",
    "    y -= mean_y\n",
    "    y /= std_y\n",
    "    \n",
    "    # x = (x - x.mean())/x.std()\n",
    "\n",
    "    return x, x_generate, y, true_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, opts):\n",
    "    \"\"\"Train a classifier on random points, i.e., a NN that learns to\n",
    "    reconstruct a function whose values are these random points\"\"\"\n",
    "    x, y = data\n",
    "\n",
    "    num_epochs = opts[\"num_epochs\"]\n",
    "    batch_size = opts[\"batch_size\"]\n",
    "    lr = opts[\"lr\"]\n",
    "    moment = opts[\"moment\"]\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=moment)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    nsamples = len(x)\n",
    "    # get indices of random samples for the batch generator\n",
    "    perm = torch.randperm(nsamples).long()\n",
    "\n",
    "    num_batches = range(1, math.ceil(nsamples / batch_size))\n",
    "\n",
    "    pbar = tqdm(enumerate(range(1, num_epochs + 1)), total=num_epochs, leave=True)\n",
    "    for i, epoch in pbar:\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in num_batches:\n",
    "            inputs = x[perm[batch*batch_size:(batch+1) * batch_size]].view(-1, 1).float()\n",
    "            targets = y[perm[batch*batch_size:(batch+1) * batch_size]].float()\n",
    "\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = F.mse_loss(output, targets, size_average=False)\n",
    "\n",
    "\n",
    "            total_loss += loss.data[0]\n",
    "            \n",
    "            # compute gradients\n",
    "            # backpropagate: update weights\n",
    "            loss.backward()        \n",
    "            optimizer.step()\n",
    "            \n",
    "        scheduler.step(total_loss)\n",
    "\n",
    "        # log partial results\n",
    "        pbar.set_description(\"epoch: {0}, avg loss: {1:.2f}\".format(epoch, total_loss/batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(activation, hid_dims, num_samples, num_epochs, batch_size, lr, moment, topk):\n",
    "    # Training settings\n",
    "    opts = {\n",
    "        \"hid_dims\": hid_dims,\n",
    "        \"acitvation\": activation,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": lr,\n",
    "        \"moment\": moment,\n",
    "    }\n",
    "\n",
    "    print(\"training model with params\")\n",
    "    for key, value in opts.items():\n",
    "        print(\"{}: {}\".format(key, value))\n",
    "\n",
    "    # Model, i.e., Neural Network\n",
    "    input_dim = 1\n",
    "    output_dim = 1\n",
    "\n",
    "    model = Approxnet(input_dim, hid_dims, output_dim, activation=activation)\n",
    "    print(model)\n",
    "\n",
    "    print(\"generating {} samples\".format(num_samples))\n",
    "    x, x_generate, y, true_func = generate_data(num_samples)\n",
    "\n",
    "    try:\n",
    "        train(model, (x_generate, y), opts)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Early stopping\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    inputs = Variable(x)\n",
    "    output = model(inputs)\n",
    "    output_hidden = model.forward(inputs, hidden=True)\n",
    "\n",
    "    plt.plot(x.squeeze().numpy(), true_func(x).squeeze().numpy(), 'g--', label=\"true function\")\n",
    "    plt.plot(x.squeeze().numpy(), output.data.squeeze().numpy(), 'r', label=\"approximation\")\n",
    "    \n",
    "    print(\"MSE approximation and true function {}\".format(torch.norm(output.data - y)))\n",
    "\n",
    "    # find neurons that contribute the most\n",
    "    # weights, hidden_idxs = torch.topk(model.output.weight, topk)\n",
    "    # weights, hidden_idxs = map(lambda x: x.data.squeeze(), [weights, hidden_idxs])\n",
    "\n",
    "    # show the function they approximate\n",
    "    # activations = []\n",
    "    # for weight, hidden_idx in zip(weights, hidden_idxs):\n",
    "    #    activation = output_hidden[:, hidden_idx].data.squeeze() * weight\n",
    "    #    activations.append(activation)\n",
    "        \n",
    "    # final_activations = activations[0]\n",
    "    # for i in range(1, len(activations)):\n",
    "    #    final_activations += activations[i]\n",
    "\n",
    "    # plt.plot(x.squeeze().numpy(), final_activations.numpy(), 'b',\n",
    "    #         label=\"{} most contributing neurons\".format(topk))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = \"sigmoid\"\n",
    "hid_dims = [20]\n",
    "num_samples = 8000\n",
    "num_epochs = 100\n",
    "batch_size = 500\n",
    "lr = 1e-5\n",
    "moment = 0.9\n",
    "topk = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b8fb788685a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "main(activation, hid_dims, num_samples, num_epochs, batch_size, lr, moment, topk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
